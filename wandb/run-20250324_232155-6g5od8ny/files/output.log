Load task dataset for [a+b mod 97]...
== Training Starts ðŸ§¨ ==
num. params in base model: [0.423296M]
num. params in amplifier: [16769]
Num. of epochs: [111111]
token_embeddings
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
  0%|                                                                                                                                                                                                                                                                                                                                                                    | 0/111111 [00:00<?, ?it/s]
*** TypeError: 'Decoder' object is not subscriptable
Embedding(101, 128)
*** AttributeError: 'Embedding' object has no attribute 'weights'
Parameter containing:
tensor([[-0.4798, -0.0970,  2.7400,  ..., -0.2192,  1.1178, -1.4389],
        [-0.8756, -0.9530, -0.5637,  ...,  0.8232, -0.3276,  1.0482],
        [-1.2733, -0.8402,  0.1546,  ...,  0.7780,  0.5690,  1.4111],
        ...,
        [-1.7685, -0.5588,  0.4917,  ...,  2.8057,  1.4309, -1.2529],
        [-0.5638, -1.1125, -1.0505,  ..., -0.6224, -0.6855,  1.5279],
        [-1.8436,  0.3365, -0.1205,  ..., -1.6355,  0.2663, -0.4066]],
       device='cuda:0', requires_grad=True)
tensor([[ 1.6474e-04, -2.9455e-04,  3.1850e-04,  ...,  4.0619e-04,
          5.1401e-04, -3.7175e-04],
        [-1.3684e-06, -1.3934e-04, -7.1024e-05,  ...,  8.6290e-05,
          2.0649e-04, -2.4214e-04],
        [-3.6725e-04,  2.1395e-04, -3.5348e-04,  ...,  1.8371e-04,
          3.6913e-04,  9.8988e-05],
        ...,
        [ 4.4422e-03, -2.1703e-03,  8.6439e-03,  ..., -4.2075e-03,
          4.0761e-03, -3.9079e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='cuda:0',
       grad_fn=<AsStridedBackward0>)
*** TypeError: 'Embedding' object is not subscriptable
*** AttributeError: 'Embedding' object has no attribute 'getattr'
Parameter containing:
tensor([[-0.4798, -0.0970,  2.7400,  ..., -0.2192,  1.1178, -1.4389],
        [-0.8756, -0.9530, -0.5637,  ...,  0.8232, -0.3276,  1.0482],
        [-1.2733, -0.8402,  0.1546,  ...,  0.7780,  0.5690,  1.4111],
        ...,
        [-1.7685, -0.5588,  0.4917,  ...,  2.8057,  1.4309, -1.2529],
        [-0.5638, -1.1125, -1.0505,  ..., -0.6224, -0.6855,  1.5279],
        [-1.8436,  0.3365, -0.1205,  ..., -1.6355,  0.2663, -0.4066]],
       device='cuda:0', requires_grad=True)
*** AttributeError: 'Embedding' object has no attribute '__attr__'
*** AttributeError: 'Embedding' object has no attribute 'in_proj_weight'
position_embeddings
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
layers
False
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
0
False
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
*** AttributeError: 'Block' object has no attribute 'in_proj_weight'
ln_1
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
*** AttributeError: 'LayerNorm' object has no attribute 'in_proj_weight'
ln_2
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
*** AttributeError: 'LayerNorm' object has no attribute 'in_proj_weight'
attn
False
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
Parameter containing:
tensor([[ 0.0165,  0.0883, -0.0728,  ..., -0.1007,  0.0649,  0.0572],
        [-0.0470, -0.0401, -0.0597,  ..., -0.0600,  0.0434,  0.0423],
        [ 0.0675,  0.1071,  0.0945,  ..., -0.0331,  0.0591, -0.0309],
        ...,
        [-0.0716, -0.0160,  0.0493,  ...,  0.0667, -0.0614,  0.0945],
        [ 0.0042, -0.0646,  0.0358,  ...,  0.0173, -0.0954,  0.0748],
        [-0.0775, -0.0615,  0.0758,  ...,  0.0564,  0.0207,  0.0496]],
       device='cuda:0', requires_grad=True)
*** AttributeError: 'MultiheadAttention' object has no attribute 'out_proj_weight'
['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_qkv_same_embed_dim', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_reset_parameters', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'add_zero_attn', 'apply', 'batch_first', 'bfloat16', 'bias_k', 'bias_v', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dropout', 'dump_patches', 'embed_dim', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'head_dim', 'in_proj_bias', 'in_proj_weight', 'ipu', 'k_proj_weight', 'kdim', 'load_state_dict', 'merge_masks', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_heads', 'out_proj', 'parameters', 'q_proj_weight', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'v_proj_weight', 'vdim', 'xpu', 'zero_grad']
Parameter containing:
tensor([[ 0.0165,  0.0883, -0.0728,  ..., -0.1007,  0.0649,  0.0572],
        [-0.0470, -0.0401, -0.0597,  ..., -0.0600,  0.0434,  0.0423],
        [ 0.0675,  0.1071,  0.0945,  ..., -0.0331,  0.0591, -0.0309],
        ...,
        [-0.0716, -0.0160,  0.0493,  ...,  0.0667, -0.0614,  0.0945],
        [ 0.0042, -0.0646,  0.0358,  ...,  0.0173, -0.0954,  0.0748],
        [-0.0775, -0.0615,  0.0758,  ...,  0.0564,  0.0207,  0.0496]],
       device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[ 0.0165,  0.0883, -0.0728,  ..., -0.1007,  0.0649,  0.0572],
        [-0.0470, -0.0401, -0.0597,  ..., -0.0600,  0.0434,  0.0423],
        [ 0.0675,  0.1071,  0.0945,  ..., -0.0331,  0.0591, -0.0309],
        ...,
        [-0.0716, -0.0160,  0.0493,  ...,  0.0667, -0.0614,  0.0945],
        [ 0.0042, -0.0646,  0.0358,  ...,  0.0173, -0.0954,  0.0748],
        [-0.0775, -0.0615,  0.0758,  ...,  0.0564,  0.0207,  0.0496]],
  0%|                                                                                                                                                                                                                                                                                                                                                                    | 0/111111 [07:12<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/homes/sfan/NeuralOptGrok/src/run.py", line 106, in <module>
    run(args)
  File "/scratch/homes/sfan/NeuralOptGrok/src/run.py", line 91, in run
    train(args,
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 103, in train
    amp_update(amp, meta_optimizer, outer_loader, model_copy, opt, inner_batch=input)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 210, in amp_update
    trans_module_weights(model_copy, amp)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 183, in trans_module_weights
    trans_module_weights(m, amp)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 183, in trans_module_weights
    trans_module_weights(m, amp)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 182, in trans_module_weights
    if len(list(m.children())) > 0:
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 182, in trans_module_weights
    if len(list(m.children())) > 0:
  File "/opt/conda/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/opt/conda/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit