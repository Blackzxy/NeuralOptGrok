Load task dataset for [a+b mod 97]...
== Training Starts ðŸ§¨ ==
num. params in base model: [0.423296M]
num. params in amplifier: [16769]
  0%|                                                                                                                                                              | 0/111111 [00:00<?, ?it/s]
Num. of epochs: [111111]
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(244)check_amp_grads()
-> for name, param in amp.named_parameters():
'network.0.weight'
Parameter containing:
tensor([[ 0.8056],
        [ 0.6439],
        [-1.7650],
        [ 0.3281],
        [-0.8101],
        [-0.7091],
        [-1.4520],
        [-2.7522],
        [-1.2782],
        [-0.6655],
        [-0.3583],
        [ 2.1525],
        [ 1.1600],
        [-0.2100],
        [-1.3817],
        [ 1.5607],
        [-0.6073],
        [-2.1334],
        [ 1.9067],
        [-0.5581],
        [-0.1525],
        [-0.6325],
        [-2.0022],
        [-2.0024],
        [ 3.1960],
        [-0.4413],
        [ 1.0851],
        [-2.9682],
        [-0.1961],
        [-1.6910],
        [-1.4210],
        [-0.3062],
        [-0.0787],
        [ 1.1498],
        [ 1.7050],
        [ 1.5519],
        [ 0.4628],
        [ 1.0304],
        [-1.7537],
        [-2.4231],
        [-0.8351],
        [ 1.3643],
        [-2.6515],
        [-1.4877],
        [-0.3103],
        [-0.7395],
        [-0.0693],
        [ 0.2623],
        [-1.9950],
        [ 2.8807],
        [ 1.7860],
        [ 1.9101],
        [ 0.2510],
        [ 0.7803],
        [-2.4163],
        [ 1.1688],
        [-0.8378],
        [ 2.2481],
        [-0.9351],
        [-0.8984],
        [-0.9475],
        [ 2.7779],
        [ 1.9237],
        [ 0.1366]], device='cuda:0', requires_grad=True)
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(244)check_amp_grads()
-> for name, param in amp.named_parameters():
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(244)check_amp_grads()
-> for name, param in amp.named_parameters():
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(244)check_amp_grads()
-> for name, param in amp.named_parameters():
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(244)check_amp_grads()
  0%|                                                                                                                                                              | 0/111111 [01:04<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/homes/sfan/NeuralOptGrok/src/run.py", line 106, in <module>
    run(args)
  File "/scratch/homes/sfan/NeuralOptGrok/src/run.py", line 91, in run
    train(args,
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 105, in train
    amp_update(amp, meta_optimizer, outer_loader, model_copy, opt, inner_batch=input, device=args.device)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 239, in amp_update
    check_amp_grads(amp)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 244, in check_amp_grads
    for name, param in amp.named_parameters():
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 244, in check_amp_grads
    for name, param in amp.named_parameters():
  File "/opt/conda/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/opt/conda/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit