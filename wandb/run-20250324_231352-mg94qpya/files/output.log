Load task dataset for [a+b mod 97]...
== Training Starts ðŸ§¨ ==
num. params in base model: [0.423296M]
num. params in amplifier: [16769]
Num. of epochs: [111111]
token_embeddings
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
  0%|                                                                                                                                                                                                                                                                                                                                                        | 0/111111 [00:00<?, ?it/s]
{'training': True, '_parameters': {'weight': Parameter containing:
tensor([[-0.4798, -0.0970,  2.7400,  ..., -0.2192,  1.1178, -1.4389],
        [-0.8756, -0.9530, -0.5637,  ...,  0.8232, -0.3276,  1.0482],
        [-1.2733, -0.8402,  0.1546,  ...,  0.7780,  0.5690,  1.4111],
        ...,
        [-1.7685, -0.5588,  0.4917,  ...,  2.8057,  1.4309, -1.2529],
        [-0.5638, -1.1125, -1.0505,  ..., -0.6224, -0.6855,  1.5279],
        [-1.8436,  0.3365, -0.1205,  ..., -1.6355,  0.2663, -0.4066]],
       device='cuda:0', requires_grad=True)}, '_buffers': {}, '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': {}, 'num_embeddings': 101, 'embedding_dim': 128, 'padding_idx': None, 'max_norm': None, 'norm_type': 2.0, 'scale_grad_by_freq': False, 'sparse': False}
False
True
'token_embeddings'
position_embeddings
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
True
layers
False
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
'layers'
False
False
0
False
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
ln_1
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
ln_2
True
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
attn
False
> /scratch/homes/sfan/NeuralOptGrok/src/train.py(182)trans_module_weights()
-> if len(list(m.children())) > 0:
'attn'
False
True
  0%|                                                                                                                                                                                                                                                                                                                                                        | 0/111111 [05:46<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/homes/sfan/NeuralOptGrok/src/run.py", line 106, in <module>
    run(args)
  File "/scratch/homes/sfan/NeuralOptGrok/src/run.py", line 91, in run
    train(args,
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 103, in train
    amp_update(amp, meta_optimizer, outer_loader, model_copy, opt, inner_batch=input)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 195, in amp_update
    trans_module_weights(model_copy, amp)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 183, in trans_module_weights
    trans_module_weights(m, amp)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 183, in trans_module_weights
    trans_module_weights(m, amp)
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 182, in trans_module_weights
    if len(list(m.children())) > 0:
  File "/scratch/homes/sfan/NeuralOptGrok/src/train.py", line 182, in trans_module_weights
    if len(list(m.children())) > 0:
  File "/opt/conda/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/opt/conda/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit